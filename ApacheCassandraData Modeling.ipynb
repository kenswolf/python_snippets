{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  /workspace/home\n",
      "\n",
      "Provided data files:\n",
      "  - /workspace/home/event_data/2018-11-10-events.csv\n",
      "  - /workspace/home/event_data/2018-11-20-events.csv\n",
      "  - /workspace/home/event_data/2018-11-09-events.csv\n",
      "  - /workspace/home/event_data/2018-11-11-events.csv\n",
      "  - /workspace/home/event_data/2018-11-07-events.csv\n",
      "  - /workspace/home/event_data/2018-11-27-events.csv\n",
      "  - /workspace/home/event_data/2018-11-21-events.csv\n",
      "  - /workspace/home/event_data/2018-11-15-events.csv\n",
      "  - /workspace/home/event_data/2018-11-17-events.csv\n",
      "  - /workspace/home/event_data/2018-11-06-events.csv\n",
      "  - /workspace/home/event_data/2018-11-24-events.csv\n",
      "  - /workspace/home/event_data/2018-11-23-events.csv\n",
      "  - /workspace/home/event_data/2018-11-04-events.csv\n",
      "  - /workspace/home/event_data/2018-11-30-events.csv\n",
      "  - /workspace/home/event_data/2018-11-29-events.csv\n",
      "  - /workspace/home/event_data/2018-11-01-events.csv\n",
      "  - /workspace/home/event_data/2018-11-16-events.csv\n",
      "  - /workspace/home/event_data/2018-11-02-events.csv\n",
      "  - /workspace/home/event_data/2018-11-13-events.csv\n",
      "  - /workspace/home/event_data/2018-11-08-events.csv\n",
      "  - /workspace/home/event_data/2018-11-12-events.csv\n",
      "  - /workspace/home/event_data/2018-11-28-events.csv\n",
      "  - /workspace/home/event_data/2018-11-19-events.csv\n",
      "  - /workspace/home/event_data/2018-11-18-events.csv\n",
      "  - /workspace/home/event_data/2018-11-26-events.csv\n",
      "  - /workspace/home/event_data/2018-11-25-events.csv\n",
      "  - /workspace/home/event_data/2018-11-05-events.csv\n",
      "  - /workspace/home/event_data/2018-11-03-events.csv\n",
      "  - /workspace/home/event_data/2018-11-14-events.csv\n",
      "  - /workspace/home/event_data/2018-11-22-events.csv\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print('current working directory: ',os.getcwd())\n",
    "\n",
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath): \n",
    "    \n",
    "    # join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*')) \n",
    "    \n",
    "print('\\nProvided data files:')\n",
    "for filename in file_path_list:\n",
    "    print('  -', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows from files (in full_data_rows_list) 8056\n"
     ]
    }
   ],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "print('Number of rows from files (in full_data_rows_list)',len(full_data_rows_list))\n",
    "# uncomment the code below if you would like to check to see what the list of event data rows will look like\n",
    "#print(full_data_rows_list)\n",
    "#print(full_data_rows_list[34])\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data rows (now in event_datafile_new.csv) 6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print('Number of data rows (now in event_datafile_new.csv)', sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Apache Cassandra coding  \n",
    "\n",
    "## Working with the CSV file titled <font color=red>event_datafile_new.csv</font>, located within the Workspace directory.  The event_datafile_new.csv contains the following columns: \n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userId\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "\n",
    "# To establish connection and begin executing queries, need a session\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyspace created\n"
     ]
    }
   ],
   "source": [
    "SQL_CREATE_KEYSPACE = \"CREATE KEYSPACE IF NOT EXISTS project_one_b_keyspace \\\n",
    "    WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 1 };\" \n",
    "\n",
    "try:\n",
    "    session.execute(SQL_CREATE_KEYSPACE)\n",
    "    print('keyspace created')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred creating the keyspace: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyspace set\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    session.set_keyspace('project_one_b_keyspace')\n",
    "    print('keyspace set')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred setting the keyspace: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1\n",
    "\n",
    "### REQUIREMENT: \n",
    "\n",
    "#### artist, song title and song's length in the music app history that was heard during  session id = 338, and item in session  = 4 \n",
    "\n",
    "### FIELDS: \n",
    "\n",
    "#### The fields in this table are the fields required for the required output (e.g. artist, song title and song's length) and the fields used for the filtering (e.g. session_id and item_in_session).  This table/query has no sorting requirements.  \n",
    "\n",
    "### PRIMARY KEY:  \n",
    "\n",
    "#### The primary key must be unique for each record in the table.  For this table/query, a unique record is an item in a session, so the primary key must include the fields session_id and item_in_session.\n",
    "\n",
    "#### The filter criteria fields are session_id and item_in_session.  The primary key must include the filter criteria fields, though only one of the fields HAS to be in the partition key.  At least one of them must be the partition key.  In this case, we definitely want to have the session_id in the partition key, but the item_in_session could be part of the partition key or it could be a clustering column.  However, since we have no sorting requirements and we have many user sessions and the sessions can include so many songs, it makes sense to use them both to parition the data.  As a result the partition key part of the primary key is a composite of the session_id and item_in_session values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table song_details\n"
     ]
    }
   ],
   "source": [
    "SQL_CREATE_SONG_DETAILS = 'CREATE TABLE IF NOT EXISTS song_details \\\n",
    "(session_id int, item_in_session int, artist_name text, song_title text, song_length float, \\\n",
    "PRIMARY KEY (session_id, item_in_session))'\n",
    "\n",
    "try:\n",
    "    session.execute(SQL_CREATE_SONG_DETAILS)\n",
    "    print('Created table song_details')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred creating the table song_details: {e}\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_FILE = 'event_datafile_new.csv'\n",
    "\n",
    "SQL_INSERT_SONG_DETAILS = \"INSERT INTO song_details \\\n",
    "(session_id, item_in_session, artist_name, song_title, song_length) \\\n",
    "values (%s,%s,%s,%s,%s)\" \n",
    "\n",
    "with open(DATA_FILE, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader: \n",
    "        session.execute(SQL_INSERT_SONG_DETAILS, \\\n",
    "                        (int(line[8]), int(line[3]), line[0], line[9], float(line[5])))\n",
    "\n",
    "print('Populated table song_details')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SQL_QUERY_SONG_DETAILS = 'SELECT artist_name, song_title, song_length FROM song_details'\n",
    "\n",
    "filter_sessionId = 338 \n",
    "filter_itemInSession = 4\n",
    "\n",
    "sql_query_song_details_with_filter = SQL_QUERY_SONG_DETAILS + \\\n",
    "f\" where session_id = {filter_sessionId} and item_in_session = {filter_itemInSession}\" \n",
    "\n",
    "try:\n",
    "    rows = session.execute(sql_query_song_details_with_filter)  \n",
    "    for row in rows:\n",
    "        print(f\"Artist: {row[0]}     Song: {row[1]}     Song Length: {row[2]}\") \n",
    "    print('\\n\\nDone with query of song_details') \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred query of the table song_details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2  \n",
    "\n",
    "### REQUIREMENT: \n",
    "\n",
    "#### name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182 \n",
    "\n",
    "### FIELDS: \n",
    "\n",
    "#### The fields in this table are the fields required for the required output (name of artist, song, and user's first and last names), the fields used for the filtering (e.g. userid and sessionid), and the fields use for the sorting (e.g. itemInSession).\n",
    "\n",
    "### PRIMARY KEY:  \n",
    "\n",
    "#### The primary key must be unique for each record in the table.  For this table/query, a unique record is an item in a session of a user, so the primary key must include those three fields, i.e. it must include item in session, session id, and user id.\n",
    "\n",
    "#### The filter criteria fields are user id and session id.  The primary key must include the filter criteria fields, though only one of the fields HAS to be in the partition key.  At least one of them must be the partition key.  In this case, because we have many users and users have may sessions, it makes sense to use them both to parition the data.  As a result the partition key part of the primary key is a composite of the user and session id values.\n",
    "\n",
    "#### The sort criteria field is the item in session.  This part of the primary key is a clustering column to insure the sort order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_CREATE_TABLE_SONG_BY_USER_SESSION = 'CREATE TABLE IF NOT EXISTS song_by_user_session \\\n",
    "(user_id int, session_id int, item_in_session int, artist_name text, song_title text, \\\n",
    "firstName text, lastName text,  \\\n",
    "PRIMARY KEY ((user_id, session_id), item_in_session) )'\n",
    "\n",
    "try: \n",
    "    session.execute(SQL_CREATE_TABLE_SONG_BY_USER_SESSION)\n",
    "    print('created table song_by_user_session')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred creating the table song_by_user_session: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_INSERT_SONG_BY_USER_SESSION = \"INSERT INTO song_by_user_session \\\n",
    "(user_id, session_id, item_in_session, artist_name, song_title, firstName, lastName) \\\n",
    "values (%s,%s,%s,%s,%s,%s,%s)\" \n",
    "\n",
    "with open(DATA_FILE, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader: \n",
    "        session.execute(SQL_INSERT_SONG_BY_USER_SESSION, \\\n",
    "                        (int(line[10]), int(line[8]), int(line[3]), line[0], line[9], line[1], line[4]) )\n",
    "\n",
    "print('Populated table song_by_user_session') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY_SONG_BY_USER_SESSION = 'SELECT artist_name, song_title, firstName, lastName FROM song_by_user_session' \n",
    "\n",
    "filter_userid = 10\n",
    "filter_itemInSession = 182\n",
    "\n",
    "sql_query_with_filter_song_in_user_session = SQL_QUERY_SONG_BY_USER_SESSION + \\\n",
    "f\" where user_id = {filter_userid} and session_id = {filter_itemInSession}\" \n",
    "\n",
    "try:\n",
    "    rows = session.execute(sql_query_with_filter_song_in_user_session) \n",
    "    \n",
    "    for row in rows:\n",
    "        print(f\"Artist: {row[0]}     Song: {row[1]}     User: {row[2]} {row[3]}\")\n",
    "\n",
    "    print('\\n\\nDone with query of song_by_user_session') \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred query of the table song_by_user_session: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3\n",
    "\n",
    "### REQUIREMENT: \n",
    "\n",
    "#### user name (first and last) in music app history who listened to the song 'All Hands Against His Own'  \n",
    "\n",
    "### FIELDS: \n",
    "\n",
    "#### The fields in this table are the fields required for the required output (e.g. user's first and last names), the fields used for the filtering (e.g. song), and the fields required to have a unique key for each row (e.g. UserIds).  We have to include the UserId field because two or more people could have the same name.  There is no sorting requirement for this query/table. \n",
    "\n",
    "### PRIMARY KEY:   \n",
    "\n",
    "#### The primary key must be unique for each record in the table.  For this table/query, a unique record is a userId and song_title.  We cannot use first and last names as part of the unique partition key because because two or more people could have the same name.   So the primary key must include the two fields userId and song_title. \n",
    "\n",
    "#### The filter criteria field is song_title so it is the partition key.   \n",
    "\n",
    "#### the userId makes the primary key unique but it is not a filter criteria so it is a clustered column.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_CREATE_TABLE_USER_BY_SONG = 'CREATE TABLE IF NOT EXISTS user_by_song \\\n",
    "(song_title text, user_id int, firstName text, lastName text, \\\n",
    "PRIMARY KEY (song_title, user_id))'\n",
    "\n",
    "try: \n",
    "    session.execute(SQL_CREATE_TABLE_USER_BY_SONG)\n",
    "    print('created table user_by_song')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred creating the table user_by_song: {e}\")                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_INSERT_USER_BY_SONG = \"INSERT INTO user_by_song (song_title, user_id, firstName, lastName) values (%s,%s,%s,%s)\" \n",
    "\n",
    "with open(DATA_FILE, encoding = 'utf8') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    next(csvreader) # skip header\n",
    "    for line in csvreader: \n",
    "        session.execute(SQL_INSERT_USER_BY_SONG, (line[9], int(line[10]), line[1], line[4]) )\n",
    "\n",
    "print('Populated table user_by_song') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_QUERY_USER_BY_SONG = 'SELECT firstName, lastName FROM user_by_song'\n",
    "\n",
    "filter_song_title = 'All Hands Against His Own'\n",
    "\n",
    "sql_query_song_user_with_filter = SQL_QUERY_USER_BY_SONG + f\" where song_title = \\'{filter_song_title}\\'\"  \n",
    "\n",
    "try:\n",
    "    rows = session.execute(sql_query_song_user_with_filter) \n",
    "     \n",
    "    for row in rows:\n",
    "        print(f\"{row[0]} {row[1]}\") \n",
    "\n",
    "    print('\\n\\nDone with query of user_by_song')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred query of the table user_by_song: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute('DROP TABLE song_details') \n",
    "    print('Dropped table song_details')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred dropping the table song_details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute('DROP TABLE song_by_user_session') \n",
    "    print('Dropped table song_by_user_session')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred dropping the table song_by_user_session: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute('DROP TABLE user_by_song') \n",
    "    print('Dropped table user_by_song')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred dropping the table user_by_song: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
